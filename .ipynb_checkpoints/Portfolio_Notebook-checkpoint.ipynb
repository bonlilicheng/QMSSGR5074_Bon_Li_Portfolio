{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249853f1",
   "metadata": {},
   "source": [
    "# Project 1: World Happiness Prediction\n",
    "\n",
    "This project aims to predict the happiness categories of countries based on factors such as GDP per capita, social support, and healthy life expectancy. The dataset used in this project is the World Happiness Competition dataset.\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "The data is preprocessed using a Sklearn Column Transformer, which includes pipelines for handling both numerical and categorical features. The preprocessing steps involve imputing missing values, scaling numerical data, and one-hot encoding categorical variables. Additionally, a heatmap of the correlation between features and happiness categories is created to visualize the relationships between variables.\n",
    "\n",
    "## Feature Selection\n",
    "\n",
    "Two feature selection techniques are applied in this project: SelectPercentile and RFE (Recursive Feature Elimination). SelectPercentile selects the top features based on their statistical scores, while RFE recursively eliminates the least important features by evaluating the performance of the models trained on the data.\n",
    "\n",
    "## Predictive Models\n",
    "\n",
    "Several predictive models are used in this project, including:\n",
    "\n",
    "- RandomForestClassifier\n",
    "- MLPClassifier (Multi-Layer Perceptron)\n",
    "- DecisionTreeClassifier\n",
    "- GradientBoostingClassifier\n",
    "\n",
    "Each of these models is trained and evaluated using the original features, as well as the features selected by SelectPercentile and RFE methods. The models' performances are compared and submitted to the competition leaderboard.\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "GridSearchCV is used to find the optimal hyperparameters for the GradientBoostingClassifier model with RFE-selected features. The hyperparameters being tuned are the number of estimators and the learning rate. The best hyperparameters are then used to train the model and make predictions.\n",
    "\n",
    "## Notebook Links\n",
    "\n",
    "- [https://github.com/bonlilicheng/QMSSGR5074_Bon_Li_Portfolio/blob/f323fee01e4d53077ce10fc594b5f0894b7226fa/Project1/HW1_World_Happiness_Classification_Competition.ipynb](#)\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "The most relevant features in predicting happiness categories are social support, GDP per capita, and healthy life expectancy. The results of the correlation analysis match the general understanding of residents' happiness. After applying feature selection techniques, hyperparameter tuning, and evaluating various models, the GradientBoostingClassifier model with RFE-selected features and optimal hyperparameters achieves the best performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b60dc00",
   "metadata": {},
   "source": [
    "# Project 2: Image Classification with Deep Learning\n",
    "\n",
    "In this project, the goal is to create an image classification model using deep learning techniques to identify different categories of images. The dataset used for this project consists of images belonging to three categories. The team has explored various models and compared their performances on the dataset. The project notebook can be found [https://github.com/bonlilicheng/QMSSGR5074_Bon_Li_Portfolio/blob/main/Project2/HW2.ipynb](#).\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset contains images of three categories. The data is preprocessed by resizing the images to 192x192 pixels, normalizing the pixel values, and one-hot encoding the labels. The preprocessed data is then split into training and testing sets.\n",
    "\n",
    "## Predictive Models\n",
    "\n",
    "The team has implemented four models for this project:\n",
    "\n",
    "1. Model 0: A basic convolutional neural network (CNN) model.\n",
    "2. Model 1: An enhanced CNN model with additional epochs, dropout layers, and a learning rate scheduler.\n",
    "3. Model 2: A transfer learning model using the VGG16 architecture pretrained on the ImageNet dataset.\n",
    "4. Model 3: A transfer learning model using the ResNet50 architecture pretrained on the ImageNet dataset.\n",
    "\n",
    "## Comparisons\n",
    "\n",
    "The models' performances are compared by submitting them to a competition leaderboard, where they are evaluated based on their accuracy on the test set. Among the models, Model 1 achieved the best results, while Models 2 and 3, which were used as transfer learning, did not perform as well.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The team has learned that transfer learning is a powerful technique to improve the performance of deep learning models. However, it requires careful attention to training parameters and dataset features to avoid overfitting, incompatible data, or incorrect architectures. It is crucial to fine-tune the pre-trained models, select the appropriate architecture, and carefully tune the hyperparameters to achieve optimal performance. The team acknowledges their limited knowledge of transfer learning and aims to utilize this technique better in the future by learning more about its nuances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e4508",
   "metadata": {},
   "source": [
    "# Project 3: Sentiment Analysis on Movie Reviews\n",
    "\n",
    "[https://github.com/bonlilicheng/QMSSGR5074_Bon_Li_Portfolio/blob/main/Project3/HW3.ipynb](#)\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset used for this project consists of movie reviews labeled as either positive or negative. The data was split into training and testing datasets, with additional fake data generated using the TextDataGenerator from the tensorflow.keras.preprocessing.text library to increase the number of training examples and improve the model's generalization.\n",
    "\n",
    "## Model 1: LSTM Model with Additional Layers and Hyperparameter Tuning\n",
    "\n",
    "The first model was an LSTM model modified from a class example. The modifications included generating more fake data, adding two LSTM layers before the dense layer, adding L2 regularization to the dense layer, and tuning the hyperparameters such as learning rate, batch size, and number of epochs. The resulting model consisted of an embedding layer, a flatten layer, two LSTM layers, two dense layers, and a dropout layer. The model achieved a high accuracy of 99.94% on the test set.\n",
    "\n",
    "## Model 2: Convolutional Neural Network (CNN) Model with Additional Layers and Hyperparameter Tuning\n",
    "\n",
    "The second model was a CNN model. It had an embedding layer followed by multiple Conv1D layers with different kernel sizes (5 and 3), MaxPooling1D layers, and a GlobalMaxPooling1D layer. The model also included a dense layer with L2 regularization, a dropout layer, and a final dense layer with a softmax activation function. The model was compiled with the Adam optimizer and trained for 20 epochs. The output shape of the Embedding layer was printed for verification purposes.\n",
    "\n",
    "## Model 3: CNN Model with Pretrained GloVe Embeddings\n",
    "\n",
    "The third model was a CNN model using pretrained GloVe embeddings for transfer learning. The model architecture was similar to Model 2, but the Embedding layer was initialized with the pretrained GloVe embeddings. The model was trained for 50 epochs with a batch size of 32.\n",
    "\n",
    "## Comparisons and Results\n",
    "\n",
    "The three models were compared using the compare_models function from the aimodelshare library. Among all the models, Model 1 (LSTM Model with Additional Layers and Hyperparameter Tuning) performed the best, achieving an accuracy of 0.7684. Model 2 and Model 3 did not perform as well as Model 1.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this project, three different models were trained on the movie reviews dataset to perform sentiment analysis. Model 1, an LSTM model with additional layers and hyperparameter tuning, outperformed the other models. The use of pretrained embeddings in Model 3 did not lead to a significant improvement in performance. The project demonstrated the importance of proper model architecture, regularization, and hyperparameter tuning in achieving high performance in sentiment analysis tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe242ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
